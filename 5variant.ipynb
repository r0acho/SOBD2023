{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1a27aa6-b53c-45d7-88bf-b7d0ee93ad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import other modules not related to PySpark\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import math\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from datetime import *\n",
    "import statistics as stats\n",
    "from functools import reduce\n",
    "\n",
    "# This helps auto print out the items without explixitly using 'print'\n",
    "InteractiveShell.ast_node_interactivity = \"all\" \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de5c02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PySpark related modules\n",
    "import pyspark\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.functions import lit, desc, col, size, array_contains\\\n",
    ", isnan, udf, hour, array_min, array_max, countDistinct\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "MAX_MEMORY = '15G'\n",
    "# Initialize a spark session.\n",
    "conf = pyspark.SparkConf().setMaster(\"local[*]\") \\\n",
    "        .set('spark.executor.heartbeatInterval', 60000) \\\n",
    "        .set('spark.network.timeout', 60000) \\\n",
    "        .set(\"spark.core.connection.ack.wait.timeout\", \"3600\") \\\n",
    "        .set(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "        .set(\"spark.driver.memory\", MAX_MEMORY)\n",
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Lab_1_var_5\") \\\n",
    "        .config(conf=conf) \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "spark = init_spark()\n",
    "\n",
    "directory_path = \"data/\"\n",
    "\n",
    "file_list = [file for file in os.listdir(directory_path) if file.endswith(\".csv\")]\n",
    "\n",
    "# Чтение файлов и создание DataFrame с явным указанием разделителя\n",
    "dataframes = [spark.read.csv(os.path.join(directory_path, file), header=True, inferSchema=True, sep=';') for file in file_list]\n",
    "\n",
    "# Объединение DataFrame в один\n",
    "merged_dataframe = reduce(DataFrame.union, dataframes)\n",
    "\n",
    "# Вывод первых строк объединенного DataFrame\n",
    "merged_dataframe.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a49819-1c42-4b1b-bffd-ebbaba29cb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f11f76-6b4e-490e-b513-a9994cde65ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаление строк с пропущенными значениями\n",
    "cleaned_dataframe = merged_dataframe.na.drop().filter(col(\"actual_consumption\") >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beadbcd-12c2-46d5-aece-d90137c806e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Рассчет квартилей\n",
    "quartiles = cleaned_dataframe.stat.approxQuantile(\"actual_consumption\", [0.25, 0.75], 0.0)\n",
    "\n",
    "# Рассчет межквартильного размаха\n",
    "IQR = quartiles[1] - quartiles[0]\n",
    "\n",
    "# Определение границ для определения выбросов\n",
    "lower_bound = quartiles[0] - 1.5 * IQR\n",
    "upper_bound = quartiles[1] + 1.5 * IQR\n",
    "\n",
    "# Удаление выбросов\n",
    "outliers_removed_dataframe = cleaned_dataframe.filter((col(\"actual_consumption\") >= lower_bound) & (col(\"actual_consumption\") <= upper_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a4bdde-88ac-4438-a6ae-33a58ecd2b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Расчет статистических показателей\n",
    "statistics_summary = cleaned_dataframe.describe()\n",
    "# Вывод сводной статистики\n",
    "statistics_summary.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeeb91c-538c-4b5b-a03a-518451677ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Преобразование Spark DataFrame в Pandas DataFrame для визуализации\n",
    "pandas_df = cleaned_dataframe.select(\"actual_consumption\").toPandas()\n",
    "\n",
    "# Построение гистограммы\n",
    "pandas_df.hist(column=\"actual_consumption\", bins=20, grid=False, edgecolor=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19836938-26fd-4ce8-87a6-e417d70220c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выбор двух столбцов для расчета корреляции\n",
    "selected_columns = [\"actual_consumption\", \"actual_pv\"]\n",
    "correlation_matrix = cleaned_dataframe.select(selected_columns).toPandas().corr()\n",
    "\n",
    "# Вывод результатов\n",
    "print(correlation_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
